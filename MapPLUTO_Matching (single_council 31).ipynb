#/usr/bin/env python
# coding: utf-8

# In[2]:


import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
import json
import geopandas as gpd
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime
from shapely.geometry import Point
import plotly.express as px


# ### Adding Block Group to all pluto

# In[3]:


df=pd.read_csv("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/MapPLUTO/pluto_22v3.csv")
df.head()


# In[4]:


df['geometry'] = df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)
df.head()


# In[5]:


gdf = gpd.GeoDataFrame(df, geometry='geometry')


# In[6]:


bg = gpd.read_file("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/Location/Block Group/tl_2017_36_bg.shp")
bg.head()


# In[7]:


bg = bg[bg["COUNTYFP"].isin(['005','047','061','081','085'])]
bg.head()


# In[8]:


#pluto with puma and bg
result = gpd.sjoin(gdf, bg, op='within')
result.head()


# In[9]:


cols_01=['bbl','unit.sqft','numfloors','unitstotal','yearbuilt',"geometry",'GEOID']
result["unit.sqft"]=result["bldgarea"]/result["unitstotal"]
result=result[cols_01]
result.head()


# In[10]:


result.rename(columns={'geometry': 'Bldg_loc', 'GEOID': 'Block_Group'}, inplace=True)


# In[11]:


gdf=gpd.GeoDataFrame(pd.merge(bg,result,left_on="GEOID", right_on = "Block_Group", how="inner"))
gdf.head()


# In[12]:


#gdf.to_csv("pluto+bg.csv")


# In[13]:


#gdf['geometry'] = gdf['geometry'].apply(lambda x: x.wkt)
#gdf.to_file("pluto_bg.shp", driver="ESRI Shapefile")


# ### Council District 31

# In[197]:


df = pd.read_csv("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/Location/pluto+Puma+BG_31.csv")
df.head()


# In[198]:


df = df[(df["council"]==31) & (df["landuse"]==1)]
df.head()


# In[199]:


df["unit.sqft"]=df["bldgarea"]/df["unitstotal"]
df["unit.sqft"].head()


# In[219]:


cols_01=['bbl','unit.sqft','numfloors','unitstotal','yearbuilt','GISJOIN','latitude','longitude','Block_Group']

cols_02=['bldg_id','in.sqft','in.geometry_stories','in.geometry_building_number_units_mf','in.vintage_acs',
         'in.geometry_building_type_recs','in.puma','out.natural_gas.total.energy_consumption_intensity',
         'out.site_energy.total.energy_consumption_intensity','in.geometry_garage','in.heating_fuel',"in.tenure",
         "in.income","in.neighbors","in.roof_material","in.geometry_wall_exterior_finish"]


# In[201]:


df_match=df[cols_01]
#df_match.to_csv("pluto_84_match.csv")
df_match.head()


# In[202]:


df_match["Block_Group"].value_counts().head()


# In[214]:


res.head()


# In[203]:


#df_match.to_csv("Pluto_BG.csv")


# In[204]:


#bg_shp=gpd.read_file("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/Location/Block Group/tl_2017_36_bg.shp")
#bg_shp.head()


# In[205]:


#merged = pd.merge(df, bg_shp, left_on='Block_Group', right_on='GEOID')
#merged.head()


# In[220]:


res = pd.read_csv("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/Metadata/Metadata_NYC_170123.csv")
res=res[res["in.geometry_garage"]=='None']
res_match=res[cols_02]
res_match.head()


# In[221]:


res_match.head()


# In[226]:


res_match[res_match["in.puma"].isin (df_match["GISJOIN"].to_list())]["in.roof_material"].value_counts()


# #### Removing end profiles with garage from ResStock

# In[206]:


res = pd.read_csv("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/Metadata/Metadata_NYC_170123.csv")
res=res[res["in.geometry_garage"]=='None']
res_match=res[cols_02]
res_match.head()


# In[ ]:


res_match["in.tenure"].value_counts()


# In[ ]:


res_match["in.geometry_building_type_recs"].value_counts()


# In[ ]:


res_match = res_match[(res_match["in.geometry_building_type_recs"] == "Single-Family Detached") | 
                      (res_match["in.geometry_building_type_recs"] == "Single-Family Attached") |
                     (res_match["in.geometry_building_type_recs"] == "Multi-Family with 2 - 4 Units")]


# In[ ]:


res_match["in.geometry_building_number_units_mf"].value_counts()


# In[ ]:


res_match = res_match[(res_match["in.geometry_building_number_units_mf"]=='None') | (res_match["in.geometry_building_number_units_mf"]=='2')]


# In[ ]:


res_match.shape


# In[ ]:


res_match.head()


# In[ ]:


df_match.head()


# In[ ]:


df_match["unitstotal"].value_counts()


# #### 1) unit sq.ft bins

# In[ ]:


sns.boxplot(res_match['in.sqft'])
plt.show()


# In[ ]:


df_match.shape


# In[ ]:


#remove inf and zero
df_match=df_match[(df_match['unit.sqft']!=np.inf) & (df_match['unit.sqft']!=0)]
df_match.shape


# In[ ]:


sns.boxplot(df_match['unit.sqft'])
plt.show()


# In[ ]:


res_match['in.sqft_bins'] = pd.qcut(res_match['in.sqft'], 12, duplicates='drop')
res_match['in.sqft_bins'].value_counts()


# In[ ]:


res_match.shape


# In[ ]:


df_match=df_match[df_match['unit.sqft'] <= 14000]


# In[ ]:


res_match.head()


# In[ ]:


bin_edges = [0,617.0,853.0,1138.0,1202.0,1623.0,1675.0,1690.0,2176.0,2663.0,14000]
df_match['in.sqft_bins'] = pd.cut(df_match['unit.sqft'], bins=bin_edges)
res_match['in.sqft_bins']= pd.cut(res_match['in.sqft'], bins=bin_edges)


# In[ ]:


res_match.head()


# In[ ]:


df_match.head()


# In[ ]:


df_match['in.sqft_bins'].value_counts()


# #### 2) Age of building

# In[ ]:


bin_edges = [0,1940,1960,1980,2000,2010,2023]
df_match['year_bins'] = pd.cut(df_match['yearbuilt'], bins=bin_edges, labels=['<1940','1940-59','1960-79','1980-99','2000-09','2010s'])
df_match.head()


# In[ ]:


res_match = res_match.rename(columns={'in.vintage_acs': 'year_bins'})


# In[ ]:


res_match.head()


# #### 3) Number of Units

# In[ ]:


res_match.head()


# In[ ]:


res_match["in.geometry_building_number_units_mf"].value_counts()


# In[ ]:


res_match["in.geometry_building_number_units_mf"]=res_match["in.geometry_building_number_units_mf"].replace("None","1")


# In[ ]:


res_match["in.geometry_building_number_units_mf"]=res_match["in.geometry_building_number_units_mf"].astype(float)


# In[ ]:


df_match["unitstotal"].value_counts()


# In[ ]:


res_match["in.geometry_building_number_units_mf"].value_counts()


# #### 4) Floors

# In[ ]:


res_match['floors_bins'] = pd.qcut(res_match['in.geometry_stories'], 10, duplicates='drop')
res_match['floors_bins'].value_counts()


# In[ ]:


df_match['floors_bins'] = pd.qcut(df_match['numfloors'], 3, duplicates='drop')
df_match['floors_bins'].value_counts()


# In[ ]:


res_match['in.geometry_stories'].value_counts()


# In[ ]:


df_match.head()


# In[ ]:


df_match['numfloors'].max()


# In[ ]:


bin_edges = [0,1.99,5]
df_match['floors_bins'] = pd.cut(df_match['numfloors'], bins=bin_edges, labels=['<2','>2'])
res_match['floors_bins'] = pd.cut(res_match['in.geometry_stories'], bins=bin_edges, labels=['<2','>2'])
df_match.head()


# In[ ]:


res_match.head()


# In[ ]:


res_match = res_match.rename(columns={'in.puma': 'Puma', 'in.geometry_building_number_units_mf': 'no_units'})
df_match = df_match.rename(columns={'GISJOIN': 'Puma', 'unitstotal': 'no_units'})


# In[ ]:


df_match.shape


# In[ ]:


#df_match.reset_index(inplace=True)
#df_match.head()


# In[ ]:


#df_match['index'].nunique()


# ### Merging Dataframes

# In[ ]:


res_plu = pd.merge(df_match, res_match, on=['Puma', 'year_bins', 'in.sqft_bins', 'no_units', 'floors_bins'],how='inner')
res_plu.head()


# ### How many load profiles were matched to each building?

# In[145]:


grouped = res_plu.groupby('bbl')['bldg_id'].apply(list)
grouped = grouped.reset_index()
grouped['num_bldg_ids'] = grouped['bldg_id'].apply(lambda x: len(x))
grouped.head(20)


# In[66]:


grouped['num_bldg_ids'].max()


# In[147]:


grouped.shape


# In[68]:


#grouped.to_csv("bbl_bldg_num.csv")


# In[159]:


df=pd.DataFrame(grouped["num_bldg_ids"].value_counts())
df = df.rename_axis('match')
df = df.rename(columns={'num_bldg_ids': 'Count'})
df = df.reset_index()
df


# In[161]:


print(df[df["match"]==1]["Count"]/df["Count"].sum()*100)


# In[158]:


res_plu["bbl"].nunique()


# ## Pruning

# ### Heating Fuel

# #### Electricity

# In[162]:


hf_bg=pd.read_csv("C:/Users/Shetty/OneDrive/Desktop/NYU/Independent Study/NREL/Validate/Queens_BG_Heating Fuel/Heating fuel_BG_without error.csv")
hf_bg.head()


# In[185]:


res.head()


# In[187]:


res["in.tenure"].value_counts()


# In[186]:


res["in.income"].nunique()


# In[184]:


hf_bg[hf_bg["Fuel oil, kerosene, etc."]==0].shape


# In[163]:


hf_bg["geoid"]=hf_bg["geoid"].str[7:]


# In[164]:


hf_bg.shape


# In[165]:


res_plu['Block_Group'] = res_plu['Block_Group'].astype(str)


# In[166]:


hf_bg = hf_bg[hf_bg['geoid'].isin(res_plu['Block_Group'].tolist())]


# In[167]:


hf_bg.shape


# In[168]:


res_plu.head()


# In[169]:


res_plu["in.heating_fuel"].value_counts()


# In[174]:


#choosing rows will more matches than 1 to optimize
df = res_plu.groupby('bbl')['bldg_id'].apply(list)
df = df.reset_index()
df['num_bldg_ids'] = df['bldg_id'].apply(lambda x: len(x))
df = df[df["num_bldg_ids"]>1]
df.head()


# In[175]:


#filtering out rows with one match
res_01 = res_plu[res_plu["bbl"].isin(df["bbl"].to_list())]


# In[176]:


merged_df = pd.merge(res_01, hf_bg, left_on='Block_Group', right_on='geoid', how='inner')

# Remove rows where heating fuel is 'electricity' and electricity is zero
merged_df = merged_df[~((merged_df['in.heating_fuel'] == 'Electricity') & (merged_df['Electricity'] == 0))]

# Keep only the columns from res_plu in the final dataframe
final_df = merged_df[res_plu.columns]


# In[177]:


grouped = final_df.groupby('bbl')['bldg_id'].apply(list)
grouped = grouped.reset_index()
grouped['num_bldg_ids'] = grouped['bldg_id'].apply(lambda x: len(x))

print(grouped['num_bldg_ids'].max())
print(grouped.shape)

grouped["num_bldg_ids"].value_counts()


# In[178]:


df=pd.DataFrame(grouped["num_bldg_ids"].value_counts())
df = df.rename_axis('match')
df = df.rename(columns={'num_bldg_ids': 'Count'})
df = df.reset_index()
df


# In[180]:


3190/(11400+3190)*100


# In[179]:


print(df[df["match"]==1]["Count"]/df["Count"].sum()*100)


# In[181]:


res_01["bbl"].nunique()


# In[182]:


13623+3190


# ### Parallel Coordinates Plot

# In[112]:


df_match_subset = df_match[['Puma', 'year_bins', 'in.sqft_bins', 'no_units', 'floors_bins', 'bbl']]
res_match_subset = res_match[['Puma', 'year_bins', 'in.sqft_bins', 'no_units', 'floors_bins', 'bldg_id']]

merged_df = pd.merge(df_match_subset, res_match_subset, on=['Puma', 'year_bins', 'in.sqft_bins', 'no_units', 'floors_bins'])


# In[113]:


merged_df.head()


# In[123]:


merged_df["bbl"].nunique()


# In[125]:


#merged_df.to_csv("merged_df.csv")


# In[114]:


import plotly.graph_objects as go
import pandas as pd


# In[137]:


merged_df.info()


# In[144]:


year_dim = go.parcats.Dimension(
    values=merged_df.year_bins.astype(str),
    categoryorder='category ascending', label="Years"
)

sqft_dim = go.parcats.Dimension(
    values=merged_df["in.sqft_bins"].astype(str),
    categoryorder='category ascending', label="Sq.ft"
)

units_dim = go.parcats.Dimension(values=merged_df['no_units'], label="No. of Units")

floors_dim = go.parcats.Dimension(
    values=merged_df["floors_bins"].astype(str),
    categoryorder='category ascending', label="Floors"
)

# Create categorical columns for bldg_id and bbl
merged_df['bldg_id'] = pd.Categorical(merged_df['bldg_id'])
merged_df['bbl'] = pd.Categorical(merged_df['bbl'])

# Define a color scale for each category
bldg_colorscale = px.colors.qualitative.Pastel
bbl_colorscale = px.colors.qualitative.Light24

# Get the intersection of the categories in merged_df['bldg_id'] and bldg_colorscale
valid_bldg_categories = merged_df['bldg_id'].cat.categories.intersection(list(bldg_colorscale))
# Map each category in bldg_id and bbl to a color using the corresponding color scale
bldg_color_mapping = dict(zip(valid_bldg_categories, pd.Series(bldg_colorscale)[valid_bldg_categories]))
bbl_categories = merged_df['bbl'].cat.categories.dropna().astype('category')
valid_bbl_categories = merged_df['bbl'].cat.categories.dropna()
valid_bbl_colorscale = bbl_colorscale[:len(valid_bbl_categories)]
bbl_color_mapping = dict(zip(valid_bbl_categories, valid_bbl_colorscale))

# Create parcats trace
color_bldg = merged_df['bldg_id'].cat.codes
color_bldg = color_bldg.apply(lambda x: bldg_color_mapping.get(merged_df['bldg_id'].cat.categories[x], 'grey'))

color_bbl = merged_df['bbl'].cat.codes
color_bbl = color_bbl.apply(lambda x: bbl_color_mapping.get(merged_df['bbl'].cat.categories[x], 'grey'))

fig = go.Figure(data=[
    go.Parcats(
        dimensions=[year_dim, sqft_dim, units_dim, floors_dim],
        line=dict(color=color_bldg, colorscale=bldg_colorscale),
        hoveron='color', hoverinfo='count+probability',
        labelfont=dict(size=18, family='Times'),
        tickfont=dict(size=16, family='Times'),
        arrangement='freeform'
    )
])

fig.update_layout(
    plot_bgcolor='white',
    paper_bgcolor='white',
    width=1500,
    height=800
)

plt.figure(figsize=(30,20))
try:
    fig.show()
except Exception as e:
    print(e)


# year_dim = go.parcats.Dimension(
#     values=merged_df.year_bins.astype(str),
#     categoryorder='category ascending', label="Years"
# )
# 
# sqft_dim = go.parcats.Dimension(
#     values=merged_df["in.sqft_bins"].astype(str),
#     categoryorder='category ascending', label="Sq.ft"
# )
# 
# units_dim = go.parcats.Dimension(values=merged_df['no_units'], label="No. of Units")
# 
# floors_dim = go.parcats.Dimension(
#     values=merged_df["floors_bins"].astype(str),
#     categoryorder='category ascending', label="Floors"
# )
# 
# bldg_dim = go.parcats.Dimension(
#     values=merged_df['bldg_id'],
#     categoryorder='category ascending', label="Building ID",
#     ticktext=merged_df['bldg_id'].unique()
# )
# 
# bbl_dim = go.parcats.Dimension(
#     values=merged_df['bbl'],
#     categoryorder='category ascending', label="BBL",
#     #ticktext=list(bbl_color_mapping.keys()),
#     #tickvals=list(bbl_color_mapping.values())
#     ticktext=merged_df['bbl'].unique()
# )
# 
# # Create categorical columns for bldg_id and bbl
# merged_df['bldg_id'] = pd.Categorical(merged_df['bldg_id'])
# merged_df['bbl'] = pd.Categorical(merged_df['bbl'])
# 
# # Define a color scale for each category
# bldg_colorscale = px.colors.qualitative.Pastel
# bbl_colorscale = px.colors.qualitative.Light24
# 
# # Get the intersection of the categories in merged_df['bldg_id'] and bldg_colorscale
# valid_bldg_categories = merged_df['bldg_id'].cat.categories.intersection(list(bldg_colorscale))
# # Map each category in bldg_id and bbl to a color using the corresponding color scale
# bldg_color_mapping = dict(zip(valid_bldg_categories, pd.Series(bldg_colorscale)[valid_bldg_categories]))
# bbl_categories = merged_df['bbl'].cat.categories.dropna().astype('category')
# valid_bbl_categories = merged_df['bbl'].cat.categories.dropna()
# valid_bbl_colorscale = bbl_colorscale[:len(valid_bbl_categories)]
# bbl_color_mapping = dict(zip(valid_bbl_categories, valid_bbl_colorscale))
# 
# # Create parcats trace
# color_bldg = merged_df['bldg_id'].cat.codes
# color_bldg = color_bldg.apply(lambda x: bldg_color_mapping.get(merged_df['bldg_id'].cat.categories[x], 'grey'))
# 
# color_bbl = merged_df['bbl'].cat.codes
# color_bbl = color_bbl.apply(lambda x: bbl_color_mapping.get(merged_df['bbl'].cat.categories[x], 'grey'))
# 
# fig = go.Figure(data=[
#     go.Parcats(
#         dimensions=[year_dim, sqft_dim, units_dim, floors_dim, bldg_dim, bbl_dim],
#         line=dict(color=color_bldg, colorscale=bldg_colorscale, cmin=0, cmax=len(bldg_color_mapping)),
#         hoveron='color', hoverinfo='count+probability',
#         labelfont=dict(size=18, family='Times'),
#         tickfont=dict(size=16, family='Times'),
#         arrangement='freeform'
#     )
# ])
# 
# fig.update_layout(
#     plot_bgcolor='white',
#     paper_bgcolor='white'
# )
# 
# plt.figure(figsize=(30,20))
# try:
#     fig.show()
# except Exception as e:
#     print(e)

# In[ ]:




